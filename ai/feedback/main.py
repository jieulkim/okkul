import os
import asyncio
import json
import time
import base64
import httpx
import numpy as np
import io
import wave
import re
from typing import List
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect, Request
from fastapi.exceptions import RequestValidationError
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from dotenv import load_dotenv
import websockets
from websockets.exceptions import ConnectionClosed
from prometheus_fastapi_instrumentator import Instrumentator

load_dotenv()

app = FastAPI(title="ì˜¤í”½ê¿€ìž¼ AI ë¶„ì„ & STT ì„œë²„")

# CORS ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€ (WebSocket ë° API ì ‘ê·¼ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Prometheus metrics endpoint for Grafana scraping
Instrumentator().instrument(app).expose(app, endpoint="/metrics")

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """
    422 ì—ëŸ¬ ë°œìƒ ì‹œ ìƒì„¸ ë¡œê·¸ë¥¼ ì¶œë ¥í•˜ëŠ” í•¸ë“¤ëŸ¬
    """
    body = await request.body()
    try:
        body_json = json.loads(body)
        print(f"âŒ [422 Error] Details: {exc.errors()}")
        print(f"âŒ [422 Error] Body Key Check: {list(body_json.keys()) if isinstance(body_json, dict) else 'Not a dict'}")
        # ì£¼ì˜: ì „ì²´ Bodyë¥¼ ì°ìœ¼ë©´ ë¡œê·¸ê°€ ë„ˆë¬´ ê¸¸ì–´ì§ˆ ìˆ˜ ìžˆìœ¼ë¯€ë¡œ, í•„ìš”ì‹œ ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©
        print(f"âŒ [422 Error] Full Body: {body_json}")
    except Exception:
        print(f"âŒ [422 Error] Body Raw: {body.decode('utf-8')}")
        
    return JSONResponse(
        status_code=422,
        content={"detail": exc.errors(), "body": str(body)},
    )

GMS_KEY = os.environ.get("GMS_KEY")
GEMINI_URL = "https://gms.ssafy.io/gmsapi/generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_REALTIME_WS_URL = os.getenv("OPENAI_REALTIME_WS_URL", "wss://api.openai.com/v1/realtime")
OPENAI_REALTIME_MODEL = os.getenv("OPENAI_REALTIME_MODEL", "gpt-4o-transcribe")
OPENAI_REALTIME_LANGUAGE = os.getenv("OPENAI_REALTIME_LANGUAGE", "en")
OPENAI_REALTIME_SAMPLE_RATE = int(os.getenv("OPENAI_REALTIME_SAMPLE_RATE", "24000"))


class AnalysisRequest(BaseModel):
    question_text: str = Field(..., description="ì˜¤í”½ ì§ˆë¬¸ í…ìŠ¤íŠ¸", example="Tell me about your favorite park.")
    user_answer: str = Field(..., description="ì‚¬ìš©ìžê°€ ë°œí™”í•œ ì˜ì–´ ë‹µë³€", example="I like Central Park...")
    user_korean_script: str = Field(..., description="ì‚¬ìš©ìžê°€ ë§í•˜ê³  ì‹¶ì—ˆë˜ í•œêµ­ì–´ ì˜ë„", example="ë‚˜ëŠ” ì„¼íŠ¸ëŸ´ íŒŒí¬ë¥¼ ì¢‹ì•„í•´. ê±°ê¸° ê°€ë©´ ë§ˆìŒì´ íŽ¸í•´ì§€ê±°ë“ .")

class SentenceFeedback(BaseModel):
    target_sentence: str = Field(..., description="ì›ë³¸ ë¬¸ìž¥")
    target_text: str = Field(..., description="ìˆ˜ì •ì´ í•„ìš”í•œ ë¶€ë¶„")
    improved_text: str = Field(..., description="êµì •ëœ ì „ì²´ ë¬¸ìž¥")
    feedback: str = Field(..., description="êµì • ì´ìœ  ë° AL íŒ")
    sentence_order: int = Field(..., description="ë¬¸ìž¥ ìˆœì„œ")

class CombinedResponse(BaseModel):
    improved_answer: str
    relevance_feedback: str
    logic_feedback: str
    fluency_feedback: str
    sentence_details: List[SentenceFeedback]

# --- ì—ì´ì „íŠ¸ í•¨ìˆ˜ ---

async def analyze_sentences_gemini(text: str):
    """1ë‹¨ê³„: ë¬¸ìž¥ ë‹¨ìœ„ êµì • ë° í•„ëŸ¬(Filler) ì£¼ìž…"""
    start_time = time.perf_counter()
    headers = {"Content-Type": "application/json", "x-goog-api-key": GMS_KEY}
    
    prompt = f"""
    ë‹¹ì‹ ì€ ì˜¤í”½ ë¬¸ìž¥ êµì • ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì•„ëž˜ ì‚¬ìš©ìž ë‹µë³€ì„ ë¶„ì„í•˜ì—¬ AL ë“±ê¸‰ì— ë§žê²Œ êµì •í•˜ì„¸ìš”.
    
    [êµì • ê°€ì´ë“œë¼ì¸]
    ì¤‘ìš”: í•´ë‹¹ ë¬¸ìž¥ì´ ì–´ëŠì •ë„ ì™„ë²½í•˜ë©´ êµì •í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    1. ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ì •ì€ ê¸°ë³¸ìž…ë‹ˆë‹¤.
    2. ë¬¸ìž¥ ì‚¬ì´ì— ìžì—°ìŠ¤ëŸ¬ìš´ Filler(you know, I mean, I gotta say, What I'm trying to say is, Well, actually I've never thought about it...) ë“±ì„ í•„ìš”í•˜ë‹¤ë©´ ì¶”ê°€í•˜ì„¸ìš”.
    3. êµ¬ì–´ì²´ í˜•íƒœë¡œ, ê°ì • í‘œí˜„ì„ ë”ìš± í’ë¶€í•˜ê²Œ ë§Œë“¤ì–´ì£¼ì„¸ìš”. (numerous, incredibly, crystal clear, stunning, go-to, laid back, relaxing, striking, challenging, various, truly, tasty ë“± ì‚¬ìš©)
    4. ë¡¤í”Œë ˆì´ë‚˜ ê³¼ê±°ì™€ í˜„ìž¬ë¥¼ ë¹„êµí•  ë• ì™„ë£Œ ì‹œì œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

    ì‘ë‹µ í˜•ì‹ (JSON):
    {{
      "sentences": [
        {{
          "target_sentence": "ì›ëž˜ ë¬¸ìž¥ ì „ì²´",
          "target_text": "ì˜¤ë¥˜/ê°œì„  êµ¬ê°„",
          "improved_text": "êµì •ëœ ë¬¸ìž¥ ì „ì²´",
          "feedback": "êµì • ì´ìœ  ë° AL íŒ (100ìž ì´ë‚´)",
          "sentence_order": 1
        }}
      ]
    }}
    ì‚¬ìš©ìž ë‹µë³€: {text}
    """
    
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"response_mime_type": "application/json"}
    }

    async with httpx.AsyncClient() as client:
        response = await client.post(GEMINI_URL, headers=headers, json=payload, timeout=30.0)
        res_json = response.json()
    
    content_text = res_json['candidates'][0]['content']['parts'][0]['text']
    sentences = json.loads(content_text).get("sentences", [])
    
    print(f"ðŸ“Š [Step 1] ë¬¸ìž¥ êµì • ì™„ë£Œ ({time.perf_counter() - start_time:.2f}s)")
    return sentences

async def analyze_overall_gemini(question: str, user_answer: str, corrected_text: str, user_korean_script: str):
    """2ë‹¨ê³„: êµì •ë³¸ìœ¼ë¡œ ëª¨ë²” ë‹µì•ˆì„ ìƒì„±í•˜ë˜, í•œêµ­ì–´ ìŠ¤í¬ë¦½íŠ¸ ëŒ€ì¡° ë° ì›ë³¸ ê¸°ì¤€ í‰ê°€ ìˆ˜í–‰"""
    start_time = time.perf_counter()
    headers = {"Content-Type": "application/json", "x-goog-api-key": GMS_KEY}
    
    prompt = f"""
    ë‹¹ì‹ ì€ ì˜¤í”½(OPIc) AL ì „ë¬¸ ì±„ì ê´€ìž…ë‹ˆë‹¤. 
    ì•„ëž˜ ì œê³µëœ [ì›ë³¸ ë‹µë³€] ë° [í•œêµ­ì–´ ì˜ë„]ë¥¼ ë¶„ì„í•˜ì—¬ í”¼ë“œë°±í•˜ê³ , [êµì •ëœ ë¬¸ìž¥ë“¤]ì„ í™œìš©í•´ ìµœì¢… í•™ìŠµìš© ëª¨ë²” ë‹µì•ˆì„ ë§Œë“œì„¸ìš”.

    [ìž…ë ¥ ë°ì´í„°]
    - ì§ˆë¬¸: {question}
    - í•œêµ­ì–´ ì˜ë„(ì‚¬ìš©ìžê°€ ë§í•˜ê³  ì‹¶ì—ˆë˜ ë‚´ìš©): {user_korean_script}
    - ì›ë³¸ ë‹µë³€(ì‚¬ìš©ìž ì‹¤ì œ ë°œí™”): {user_answer}
    - êµì •ëœ ë¬¸ìž¥ë“¤(1ë‹¨ê³„ ê²°ê³¼ë¬¼): {corrected_text}

    [ìž‘ì„± ê·œì¹™ - ë°˜ë“œì‹œ JSON í¬ë§·ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”]
    1. improved_answer: [êµì •ëœ ë¬¸ìž¥ë“¤]ì˜ ë‚´ìš©ì„ ìœ ì§€í•˜ë©°, ì „ì²´ íë¦„ì´ ìžì—°ìŠ¤ëŸ½ë„ë¡ ì—°ê²° ì–´êµ¬ë§Œ ì¶”ê°€í•˜ì—¬ ì™„ì„±í•˜ì„¸ìš”.
       - ì„œë‘ì— í™•ì‹¤í•œ Main Point(MP)ê°€ ë“œëŸ¬ë‚˜ì•¼ í•©ë‹ˆë‹¤.
       - í•œêµ­ì–´ ì˜ë„ì—ëŠ” ìžˆìœ¼ë‚˜ ì˜ì–´ ë‹µë³€ì—ì„œ ëˆ„ë½ëœ í•µì‹¬ ë‚´ìš©ì´ ìžˆë‹¤ë©´ ìžì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œì¼œ ì™„ì„±í•˜ì„¸ìš”.
       - COMBO ë¬¸ì œê±°ë‚˜, ì´ì „ì— ë§í•œ ë‚´ìš©ì´ ì–¸ê¸‰ë  ë• "As I told you before"ì™€ ê°™ì€ ì—°ê²° ê³ ë¦¬ë¥¼ ë„£ìœ¼ì„¸ìš”.
       - ê·¸ ì™¸ì— ë¬¸ìž¥ ì—°ê²° ê°„ì— í•„ìš”í•œ í•„ëŸ¬ë¥¼ ë¬¸ë§¥ìƒ ì ì ˆížˆ ì‚¬ìš©í•˜ì„¸ìš”. (I think that's all I can say about me, That's all I wanted to say, What I'm trying to say, To put detail~ , At the end of the day, Or something, Obviously, Currently, basically, You see, I mean, In fact, what else- , What I really love about is that, The reason why, what am I trying to say, anyway, I gotta tell you, Wow... It's quite a tough question, That's tricky, That is a reason why)
       - ì•ž ë’¤ ë¬¸ìž¥ì˜ ë§¥ë½ì´ ë‹¬ë¼ì§ˆ ë• By the way ë“±ì˜ ì ‘ì†ì‚¬ë¥¼ ì ì ˆí•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”. 

    2. relevance_feedback: [ì›ë³¸ ë‹µë³€]ì´ ì§ˆë¬¸ì˜ ì˜ë„ì— ë¶€í•©í•˜ëŠ”ì§€ í•œêµ­ì–´ë¡œ í‰ê°€í•˜ì„¸ìš”. ë°˜ë“œì‹œ ì›ë³¸ ë‹µë³€ ê¸°ë°˜ìœ¼ë¡œ. 
       (êµì •ë³¸ì´ ì•„ë‹Œ, ì‚¬ìš©ìžê°€ ì²˜ìŒì— ë§í•œ ë‚´ìš©ì´ ì§ˆë¬¸ì— ë§žëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.)

    3. logic_feedback: [ì›ë³¸ ë‹µë³€]ì˜ ë…¼ë¦¬ ì „ê°œë¥¼ í•œêµ­ì–´ë¡œ í‰ê°€í•˜ì„¸ìš”. ë°˜ë“œì‹œ ì›ë³¸ ë‹µë³€ ê¸°ë°˜ìœ¼ë¡œ.
       - ì˜¤í”½ì—ì„œ ë†’ì€ ë“±ê¸‰(IH/AL)ì„ ë°›ìœ¼ë ¤ë©´ ë‹¨ìˆœ ë¬¸ìž¥ ë‚˜ì—´ì´ ì•„ë‹Œ, Main Point(í•µì‹¬ ë¬¸ìž¥)ë¥¼ ë¨¼ì € ì œì‹œí•˜ê³  ê·¸ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê·¼ê±°, ê²½í—˜, ë¬˜ì‚¬, ê²°ë¡ ì„ ë§ë¶™ì´ëŠ” êµ¬ì¡°í™”ëœ ë…¼ë¦¬ì  ë‹µë³€ì´ í•„ìˆ˜ì ìž…ë‹ˆë‹¤. ì´ì— ê´€í•´ ì›ë³¸ ë‹µë³€ì´ ë¶€ì¡±í•œ ë¶€ë¶„ì„ í”¼ë“œë°±í•˜ì„¸ìš”.
       - ì›ë³¸ ë‹µë³€ì´ 'ì£¼ì œì— ëŒ€í•œ ë‹µë³€, ë‹¹ì‹œì˜ ê°ì •, ì´ìœ 'ì˜ êµ¬ì¡°ì¸ì§€ ëŸ¬í”„í•˜ê²Œ í™•ì¸í•˜ê³ , ê·¸ê²Œ ì•„ë‹ˆë¼ë©´ ê·¸ë ‡ê²Œ ê³ ì¹˜ëŠ”ê²Œ ì¢‹ë‹¤ê³  ì¡°ì–¸í•˜ì„¸ìš”. ë¹„ìŠ·í•œ êµ¬ì¡°ë¥¼ ë”°ë¥¸ë‹¤ë©´ ì¹­ì°¬ë§Œ í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.

    4. fluency_feedback: [ì›ë³¸ ë‹µë³€]ì˜ ë°œí™”ëŸ‰ê³¼ ìœ ì°½ì„±ì„ í•œêµ­ì–´ë¡œ í‰ê°€í•˜ì„¸ìš”. ë°˜ë“œì‹œ ì›ë³¸ ë‹µë³€ ê¸°ë°˜ìœ¼ë¡œ.
       - [í•œêµ­ì–´ ì˜ë„]ì™€ ë¹„êµí–ˆì„ ë•Œ ì˜ì–´ ë‹µë³€ì—ì„œ ë¹ ì§„ ë¶€ë¶„ì´ë‚˜ ì™œê³¡ëœ ë‚´ìš©ì´ ìžˆëŠ”ì§€ ëŒ€ì¡° ë¶„ì„ì„ í¬í•¨í•˜ì„¸ìš”.
       - ë§Œì•½ [í•œêµ­ì–´ ì˜ë„]ê°€ ë¹„ì–´ìžˆë‹¤ë©´, [ì›ë³¸ ë‹µë³€]ì˜ ë¶„ëŸ‰ê³¼, ì–´íœ˜ì˜ ì§€ë‚˜ì¹œ ë°˜ë³µ ì—¬ë¶€ë§Œìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.
       - 2ë¬¸ìž¥ ì´í•˜: ì‹¬ê°í•œ ì§€ì , 4ë¬¸ìž¥ ì´í•˜: ë³´ê°• ì¡°ì–¸, 5ë¬¸ìž¥ ì´ìƒ: ì¹­ì°¬.
       - í‘œí˜„ì˜ ë‹¤ì–‘ì„±ì€ ì§€ë‚˜ì¹˜ê²Œ ì—„ê²©í•˜ì§€ ì•Šê²Œ, ê²©ë ¤ ìœ„ì£¼ë¡œ ìž‘ì„±í•˜ì„¸ìš”. ë„ˆë¬´ ë‹¨ì¡°ë¡­ë‹¤ë©´ ê·¸ë•Œë§Œ ì§€ì í•˜ì„¸ìš”.
    
    ì£¼ì˜: relevance_feedback, logic_feedback, fluency_feedbackì€ ì ˆëŒ€ë¡œ [êµì •ëœ ë¬¸ìž¥ë“¤] ê¸°ì¤€ì´ ì•„ë‹Œ, [ì›ë³¸ ë‹µë³€]ì˜ ìˆ˜ì¤€ì„ ë°”íƒ•ìœ¼ë¡œ ìž‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. fluency_feedback í•­ëª©ì€ [ì›ë³¸ ë‹µë³€]ê³¼ [í•œêµ­ì–´ ì˜ë„] ì‚¬ì´ì˜ ê°„ê·¹ë„ ì²´í¬í•˜ì„¸ìš”.
    """
    
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {
            "response_mime_type": "application/json",
            "response_schema": {
                "type": "object",
                "properties": {
                    "improved_answer": {"type": "string"},
                    "relevance_feedback": {"type": "string"},
                    "logic_feedback": {"type": "string"},
                    "fluency_feedback": {"type": "string"}
                },
                "required": ["improved_answer", "relevance_feedback", "logic_feedback", "fluency_feedback"]
            }
        }
    }

    async with httpx.AsyncClient() as client:
        response = await client.post(GEMINI_URL, headers=headers, json=payload, timeout=30.0)
        res_json = response.json()
    
    print(f"ðŸ“Š [Step 2] ì›ë³¸ ë° ì˜ë„ ëŒ€ì¡° ì¢…í•© í”¼ë“œë°± ì™„ë£Œ ({time.perf_counter() - start_time:.2f}s)")
    return json.loads(res_json['candidates'][0]['content']['parts'][0]['text'])

# --- API ì—”ë“œí¬ì¸íŠ¸ ---

@app.post(
    "/v1/analyze", 
    response_model=CombinedResponse,
    tags=["Practice"],
    summary="ì‚¬ìš©ìž ë‹µë³€ AI ì¢…í•© ë¶„ì„",
    description="ì‚¬ìš©ìžì˜ ì˜ì–´ ë‹µë³€ê³¼ í•œêµ­ì–´ ì˜ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¬¸ë²• êµì • ë° AL ê¸°ì¤€ í”¼ë“œë°±ì„ ìƒì„±í•©ë‹ˆë‹¤."
)
async def analyze_voice_text(request: AnalysisRequest):
    total_start = time.perf_counter()
    try:
        # 1. ë¬¸ë²• ë° í•„ëŸ¬ êµì •
        sentence_details = await analyze_sentences_gemini(request.user_answer)
        
        # êµì •ëœ ë¬¸ìž¥ë“¤ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        corrected_text = " ".join([s['improved_text'] for s in sentence_details])
        
        # 2. í•œêµ­ì–´ ì˜ë„ ëŒ€ì¡° ë° ì¢…í•© í”¼ë“œë°± ìƒì„± (ìˆœì°¨ ì‹¤í–‰)
        overall_res = await analyze_overall_gemini(
            request.question_text, 
            request.user_answer, 
            corrected_text,
            request.user_korean_script
        )
        
        total_duration = time.perf_counter() - total_start
        print(f"âœ¨ [ì „ì²´ ë¶„ì„ ì™„ë£Œ] ì´ ì†Œìš” ì‹œê°„: {total_duration:.2f}s")
        
        return {**overall_res, "sentence_details": sentence_details}
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# --- ë¼ìš°í„° ë“±ë¡ ---
from exam_feedback import router as exam_router
from total_feedback import router as total_router

app.include_router(exam_router, prefix="/v1")
app.include_router(total_router, prefix="/v1")

# ==========================================
# OpenAI Realtime Transcription & WebSocket Logic
# ==========================================

def _build_context_tail(existing: str, incoming: str, max_words: int = 32) -> str:
    merged = " ".join([part.strip() for part in [existing, incoming] if part and part.strip()])
    if not merged:
        return ""
    words = merged.split()
    return " ".join(words[-max_words:])

# whister API ë¬¸ì œë¡œ ë°œìƒí•˜ëŠ” ë¶ˆí•„ìš”í•œ ë°œí™” í•„í„°ë§
def _is_unwanted_transcript(text: str) -> bool:
    normalized = (text or "").strip().lower()
    if not normalized:
        return True
    blocked_patterns = [
        r"\bwww\.",
        r"\.com\b",
        r"\bengvid\b",
        r"\bsubscribe\b",
        r"\blike and subscribe\b",
        r"\bfree course\b",
    ]
    return any(re.search(pattern, normalized) for pattern in blocked_patterns)


def _segment_is_valid(audio_len_bytes: int, voiced_samples: int, total_samples: int, min_audio_bytes: int) -> bool:
    if audio_len_bytes <= min_audio_bytes:
        return False
    if total_samples <= 0:
        return False
    voiced_ratio = voiced_samples / total_samples
    if voiced_samples < int(16000 * 0.35):
        return False
    if voiced_ratio < 0.22:
        return False
    return True


async def transcribe_audio_async(audio_bytes, prompt_text: str = ""):
    """
    ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ OpenAI APIë¥¼ í˜¸ì¶œí•˜ì—¬ STTë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    try:
        buffer = io.BytesIO()
        buffer.name = "audio.wav"
        
        with wave.open(buffer, 'wb') as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2) 
            wf.setframerate(16000)
            wf.writeframes(audio_bytes)
        
        buffer.seek(0)
        start_time = time.time()
        
        prompt = normalize_prompt = (prompt_text or "").strip()
        if prompt:
            prompt = (
                "This is a continuous spoken English answer. "
                f"Keep lexical continuity with previous context: {normalize_prompt}"
            )

        transcript = await client.audio.transcriptions.create(
            model="whisper-1",
            file=buffer,
            language="en",
            prompt=prompt if prompt else None,
        )
        print(f"ðŸš€ [Whisper] OpenAI ì‘ë‹µ ì™„ë£Œ ({time.time() - start_time:.2f}s)")
        return transcript.text


@app.websocket("/ws/transcribe")
async def websocket_endpoint(websocket: WebSocket):
    if not OPENAI_API_KEY:
        await websocket.close(code=1011, reason="OPENAI_API_KEY is missing")
        return

    await websocket.accept()
    print("[WS] Client Connected")

    audio_buffer = bytearray()
    silence_start_time = None
    is_speaking = False
    segment_seq = 0
    packet_count = 0
    transcript_context = ""
    voiced_samples = 0
    total_samples = 0

    # VAD parameters
    SILENCE_THRESHOLD = 0.1
    SILENCE_DURATION = 1
    MIN_AUDIO_LENGTH = int(16000 * 2 * 0.6)

    try:
        while True:
            message = await websocket.receive()
            if message.get("type") == "websocket.disconnect":
                raise WebSocketDisconnect()

            text_payload = message.get("text")
            if text_payload is not None:
                try:
                    payload = json.loads(text_payload)
                except json.JSONDecodeError:
                    continue

                if payload.get("event") == "eof":
                    print(
                        f"[WS] EOF received. Flushing... buffer_bytes={len(audio_buffer)} seq={segment_seq} "
                        f"voiced_samples={voiced_samples} total_samples={total_samples}"
                    )
                    if len(audio_buffer) > 0 and _segment_is_valid(
                        len(audio_buffer), voiced_samples, total_samples, MIN_AUDIO_LENGTH
                    ):
                        flush_start = time.time()
                        text = await transcribe_audio_async(audio_buffer, transcript_context)
                        if text and not _is_unwanted_transcript(text):
                            segment_seq += 1
                            transcript_context = _build_context_tail(transcript_context, text)
                            print(f"[STT Result#{segment_seq}] flush chars={len(text)} took={time.time() - flush_start:.2f}s")
                            await websocket.send_json({"type": "full", "seq": segment_seq, "text": text})
                        elif text:
                            print(f"[STT Filtered] flush text dropped: {text}")
                    elif len(audio_buffer) > 0:
                        print(
                            f"[Talk] EOF segment skipped by quality gate: "
                            f"bytes={len(audio_buffer)} voiced={voiced_samples} total={total_samples}"
                        )

                    print(f"[WS] Sending done seq={segment_seq}")
                    await websocket.send_json({"type": "done", "seq": segment_seq})
                    audio_buffer = bytearray()
                    is_speaking = False
                    silence_start_time = None
                    voiced_samples = 0
                    total_samples = 0
                continue

            audio_bytes = message.get("bytes")
            if audio_bytes is None:
                continue

            packet_count += 1
            if packet_count % 200 == 0:
                print(f"[WS] packets={packet_count} buffer_bytes={len(audio_buffer)} speaking={is_speaking}")

            audio_float32 = np.frombuffer(audio_bytes, dtype=np.float32)
            audio_int16 = (audio_float32 * 32767).astype(np.int16)
            volume = np.max(np.abs(audio_float32)) if len(audio_float32) > 0 else 0

            if volume > SILENCE_THRESHOLD:
                if not is_speaking:
                    is_speaking = True
                    print(f"[Talk] Speaking started... packet={packet_count}")
                silence_start_time = None
                audio_buffer.extend(audio_int16.tobytes())
                voiced_samples += len(audio_int16)
                total_samples += len(audio_int16)
            else:
                if is_speaking:
                    audio_buffer.extend(audio_int16.tobytes())
                    total_samples += len(audio_int16)
                    if silence_start_time is None:
                        silence_start_time = time.time()
                    elif time.time() - silence_start_time > SILENCE_DURATION:
                        print(
                            f"[Talk] Sentence ended. Requesting Whisper... buffer_bytes={len(audio_buffer)} "
                            f"voiced_samples={voiced_samples} total_samples={total_samples}"
                        )

                        if _segment_is_valid(len(audio_buffer), voiced_samples, total_samples, MIN_AUDIO_LENGTH):
                            segment_start = time.time()
                            text = await transcribe_audio_async(audio_buffer, transcript_context)

                            if text and not _is_unwanted_transcript(text):
                                segment_seq += 1
                                transcript_context = _build_context_tail(transcript_context, text)
                                print(f"[STT Result#{segment_seq}] segment chars={len(text)} took={time.time() - segment_start:.2f}s")
                                await websocket.send_json({"type": "full", "seq": segment_seq, "text": text})
                            elif text:
                                print(f"[STT Filtered] segment text dropped: {text}")
                        else:
                            print(
                                f"[Talk] Segment skipped by quality gate: "
                                f"bytes={len(audio_buffer)} voiced={voiced_samples} total={total_samples}"
                            )

                        audio_buffer = bytearray()
                        is_speaking = False
                        silence_start_time = None
                        voiced_samples = 0
                        total_samples = 0

    except WebSocketDisconnect:
        print("[WS] Client Disconnected")
    except Exception as e:
        print(f"[WS] Error: {e}")
